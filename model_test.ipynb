{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e9cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TextIteratorStreamer\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac48358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 1792, padding_idx=128001)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1792, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1792, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1792, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1792, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1792, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1792, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=3072, out_features=1792, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=1792, out_features=8064, bias=False)\n",
       "              (up_proj): Linear(in_features=1792, out_features=8064, bias=False)\n",
       "              (down_proj): Linear(in_features=8064, out_features=1792, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((1792,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((1792,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((1792,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1792, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dir = \"../model/kanana-nano-2.1b-instruct\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, padding_side=\"left\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./outputs/checkpoint-400\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f767843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_template = \"\"\"{% for message in messages %}\n",
    "{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}\n",
    "<|start_header_id|>{{ message['role'] }}<|end_header_id|>\n",
    "\n",
    "{% if message['role'] == 'assistant' %}\n",
    "{% generation %}\n",
    "{{ message['content'] | trim }}\n",
    "{% endgeneration %}\n",
    "{% else %}\n",
    "{{ message['content'] | trim }}\n",
    "{% endif %}\n",
    "<|eot_id|>\n",
    "{% endfor %}\n",
    "\n",
    "{% if add_generation_prompt %}\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "tokenizer.chat_template = new_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe1322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=1024,  # 생성할 최대 토큰 수 (작게 조정)\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 끝 토큰 명시\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1600836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_with_langchain(question):\n",
    "    \"\"\"LangChain과 스트리밍을 함께 사용\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "    \n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"streamer\": streamer,\n",
    "    }\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "    \n",
    "    thread.join()\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac50370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_base_model(question):\n",
    "    prompt = question  # 이것이 전부입니다\n",
    "    \n",
    "    input_ids = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,\n",
    "        skip_prompt=True\n",
    "    )\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": input_ids[\"input_ids\"],\n",
    "        \"generation_config\": generation_config,\n",
    "        \"streamer\": streamer,\n",
    "    }\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "\n",
    "    thread.join()\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a65ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 네가 시험을 망쳐서 슬프시겠어요. 부모님께 혼나서 더 슬프시겠어요. 어떤 말씀을 하셨나요? 부모님께 위로받고 싶으신가요?\n",
      "혹시 부모님께서 그런 말을 해주셨나요?\n",
      "부모님께서 힘내라고 응원해 주셨나요?\n",
      "부모님께서 응원해 주셨으면 좋겠어요.\n",
      "부모님께서 응원해 주셔서 힘내시길 바라요. 지금의 슬픔을 부모님께서 위로해 주셨으면 좋겠어요.\n",
      "오늘 하루도 힘내세요! 힘내세요! 힘내세요! \n",
      "항상 응원할게요.  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! 힘내세요!  힘내세요! 힘내세요! "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA: \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstream_with_langchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m, in \u001b[0;36mstream_with_langchain\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     23\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     25\u001b[0m full_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m streamer:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(text, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m     full_response \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\generation\\streamers.py:226\u001b[0m, in \u001b[0;36mTextIteratorStreamer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 226\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_signal:\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\llm_env\\lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\llm_env\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-base 알파카 탬플릿 kubeflow 60 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: AI 플랫폼을 소개해주세요.\n",
      "\n",
      "A: AI 개발을 위한 도구와 리소스를 제공하는 플랫폼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-base 알파카 탬플릿 kubeflow 60 스텝 학습\n",
    "question = \"AI 플랫폼을 소개해주세요.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: AI 플랫폼을 소개해주세요.\n",
      "\n",
      "A: AI 플랫폼을 소개하겠습니다.\n",
      "\n",
      "### 카카오클라우드 AI 서비스\n",
      "\n",
      "카카오클라우드는 사용자 중심의 디지털 혁신을 위해 AI 서비스를 제공합니다. 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **카카오클라우드 AI 서비스(Develop)**:\n",
      "   - AI 개발을 위한 도구와 리소스를 제공하여, 사용자가 머신러닝 모델을 쉽게 구축하고 테스트할 수 있도록 지원합니다.\n",
      "   - Jupyter Notebook을 통해 코드 실행 및 시각화를 통해 머신러닝 모델을 구현하고 최적화할 수 있습니다.\n",
      "\n",
      "2. **카카오클라우드 AI 서비스(Operate)**:\n",
      "   - AI 모델 운영을 지원하는 도구와 서비스를 제공합니다. 예를 들어, Kubeflow를 통해 머신러닝 워크플로우를 구성하고 실행할 수 있습니다.\n",
      "   - AI 모델의 배포, 모니터링, 자동화 등을 효율적으로 관리할 수 있습니다.\n",
      "\n",
      "3. **카카오클라우드 AI 서비스(Explain)**:\n",
      "   - 모델의 예측을 설명하고 투명성을 제공하는 도구와 서비스를 제공합니다. SHAP (SHapley Value-based Image Attentions)를 통해 모델의 예측 과정을 분석할 수 있습니다.\n",
      "\n",
      "4. **카카오클라우드 AI 서비스(Train)**:\n",
      "   - 대규모 데이터셋을 빠르고 효율적으로 처리할 수 있는 GPU 기반의 컴퓨팅 리소스를 제공합니다.\n",
      "   - 다양한 머신러닝 프레임워크와 라이브러리를 지원하여, 모델을 쉽게 개발하고 실험할 수 있습니다.\n",
      "\n",
      "카카오클라우드 AI 서비스는 개발자와 기업이 머신러닝 모델을 효율적으로 구축하고 운영할 수 있도록 다양한 리소스와 도구를 제공합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 알파카 탬플릿 kubeflow 10 에포크 학습\n",
    "question = \"AI 플랫폼을 소개해주세요.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15782573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 오늘 시험을 망쳐서 부모님께 혼나셨군요. 너무 속상하고 슬프셨겠어요. 부모님께 혼나서 슬픈 마음이 풀릴 수 있을까요?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 알파카 탬플릿 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b24e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 저녁 메뉴 추천해줘.\n",
      "\n",
      "A: 오늘 저녁 메뉴로 추천드리는 것은 \"돼지갈비찜\"입니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 챗 탬플릿 감정 대화 50 스텝 학습\n",
    "question = \"오늘 저녁 메뉴 추천해줘.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cd09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 부모님께 혼나서 기분이 좋지 않으신 것 같아요.\n",
      "무슨 일이 있었나요?\n",
      "부모님께 혼난 이유를 말씀해주실 수 있나요?\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바�\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 챗 탬플릿 assistant only 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d95e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 많이 속상하시겠어요.\n",
      "오늘 시험이 왜 그런 일이 생겼나요?\n",
      "그러한 일이 생기면 어떻게 하면 좋을까요?\n",
      "부모님께 속상한 마음을 말씀드릴 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "어떤 방법으로 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "어떤 방법으로 기분을 풀 수 있을까요?\n",
      "또한 부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "또한 부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 챗 탬플릿 assistant only 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f919f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 어머, 시험을 망쳐서 많이 슬프겠네요. 그래도 부모님께 혼나서 많이 혼나셨겠어요. 기분이 나아지도록 어떻게 하면 좋을까요? 같이 이야기 나눠봐요. 그래도 부모님께서 기운 내라고 응원해 주실 거예요. 힘내세요!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 알파카 탬플릿 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4952dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 그런 상황이 참 안타깝네요. 시험 결과는 누구에게나 힘들 수 있는 부분이지만, 그로 인해 부모님께 혼나는 것은 정말 마음 아플 것 같아요. \n",
      "\n",
      "부모님께서도 너의 감정을 이해하려고 노력하시면 좋을 것 같아요. 혼내시더라도 시험 결과에 대한 구체적인 피드백을 주시고, 앞으로 어떻게 개선할 수 있을지 함께 고민해보면 좋겠어요. \n",
      "\n",
      "또한, 스스로를 너무 비난하지 말고, 다음에는 더 잘할 수 있을 거라는 자신감을 가지는 것도 중요해요. 힘든 시간이겠지만, 주변 사람들의 지지와 격려가 큰 힘이 될 거예요.\n",
      "\n",
      "혹시 친구나 선생님과 이야기하고 싶다면, 그들도 너를 이해하고 도와줄 수 있을 거예요. 슬픈 감정을 털어놓고 함께 해결책을 찾아보는 것도 좋은 방법이에요.\n",
      "\n",
      "기분이 나아질 수 있는 방법을 찾아보거나, 좋아하는 활동을 하면서 기분을 전환하는 것도 도움이 될 수 있어요. \n",
      "\n",
      "언제든지 이야기할 수 있는 사람이 있다는 것을 기억하고, 힘든 시간을 잘 이겨내길 바랍니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 알파카 탬플릿 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456947d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
