{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e9cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TextIteratorStreamer\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac48358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c979d9abb624c3b86e8f5174f67d453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dir = \"../model/gemma-ko-2b-instruct-v0.51\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, padding_side=\"left\")\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     base_model,\n",
    "#     \"./outputs/checkpoint-2000\"\n",
    "# )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f767843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_template = \"\"\"{% for message in messages %}\n",
    "{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}\n",
    "<|start_header_id|>{{ message['role'] }}<|end_header_id|>\n",
    "\n",
    "{% if message['role'] == 'assistant' %}\n",
    "{% generation %}\n",
    "{{ message['content'] | trim }}\n",
    "{% endgeneration %}\n",
    "{% else %}\n",
    "{{ message['content'] | trim }}\n",
    "{% endif %}\n",
    "<|eot_id|>\n",
    "{% endfor %}\n",
    "\n",
    "{% if add_generation_prompt %}\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "tokenizer.chat_template = new_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe1322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=1024,  # 생성할 최대 토큰 수 (작게 조정)\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 끝 토큰 명시\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1600836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_with_langchain(question):\n",
    "    \"\"\"LangChain과 스트리밍을 함께 사용\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "    \n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"generation_config\": generation_config,\n",
    "        \"streamer\": streamer,\n",
    "    }\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "    \n",
    "    thread.join()\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac50370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_base_model(question):\n",
    "    prompt = question  # 이것이 전부입니다\n",
    "    \n",
    "    input_ids = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,\n",
    "        skip_prompt=True\n",
    "    )\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": input_ids[\"input_ids\"],\n",
    "        \"generation_config\": generation_config,\n",
    "        \"streamer\": streamer,\n",
    "    }\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    full_response = \"\"\n",
    "    for text in streamer:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "\n",
    "    thread.join()\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a65ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 저녁 메뉴를 하나 추천해줘.\n",
      "\n",
      "A: 샐러드나 섬유유연제, 그리고 염색제를 사용하는 것이 좋을 것 같아요. 이러한 제품들은 세균과 곰팡이를 증가시키기 때문에 피부 건강에 좋지 않아요. 또한, 섬유 유연제는 옷감을 상하게 할 수 있으므로 염색제를 사용하는 것을 추천합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-base 알파카 탬플릿 kubeflow 60 스텝 학습\n",
    "question = \"저녁 메뉴를 하나 추천해줘.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: AI 플랫폼을 소개해주세요.\n",
      "\n",
      "A: AI 개발을 위한 도구와 리소스를 제공하는 플랫폼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-base 알파카 탬플릿 kubeflow 60 스텝 학습\n",
    "question = \"AI 플랫폼을 소개해주세요.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: AI 플랫폼을 소개해주세요.\n",
      "\n",
      "A: AI 플랫폼을 소개하겠습니다.\n",
      "\n",
      "### 카카오클라우드 AI 서비스\n",
      "\n",
      "카카오클라우드는 사용자 중심의 디지털 혁신을 위해 AI 서비스를 제공합니다. 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **카카오클라우드 AI 서비스(Develop)**:\n",
      "   - AI 개발을 위한 도구와 리소스를 제공하여, 사용자가 머신러닝 모델을 쉽게 구축하고 테스트할 수 있도록 지원합니다.\n",
      "   - Jupyter Notebook을 통해 코드 실행 및 시각화를 통해 머신러닝 모델을 구현하고 최적화할 수 있습니다.\n",
      "\n",
      "2. **카카오클라우드 AI 서비스(Operate)**:\n",
      "   - AI 모델 운영을 지원하는 도구와 서비스를 제공합니다. 예를 들어, Kubeflow를 통해 머신러닝 워크플로우를 구성하고 실행할 수 있습니다.\n",
      "   - AI 모델의 배포, 모니터링, 자동화 등을 효율적으로 관리할 수 있습니다.\n",
      "\n",
      "3. **카카오클라우드 AI 서비스(Explain)**:\n",
      "   - 모델의 예측을 설명하고 투명성을 제공하는 도구와 서비스를 제공합니다. SHAP (SHapley Value-based Image Attentions)를 통해 모델의 예측 과정을 분석할 수 있습니다.\n",
      "\n",
      "4. **카카오클라우드 AI 서비스(Train)**:\n",
      "   - 대규모 데이터셋을 빠르고 효율적으로 처리할 수 있는 GPU 기반의 컴퓨팅 리소스를 제공합니다.\n",
      "   - 다양한 머신러닝 프레임워크와 라이브러리를 지원하여, 모델을 쉽게 개발하고 실험할 수 있습니다.\n",
      "\n",
      "카카오클라우드 AI 서비스는 개발자와 기업이 머신러닝 모델을 효율적으로 구축하고 운영할 수 있도록 다양한 리소스와 도구를 제공합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 알파카 탬플릿 kubeflow 10 에포크 학습\n",
    "question = \"AI 플랫폼을 소개해주세요.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15782573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 오늘 시험을 망쳐서 부모님께 혼나셨군요. 너무 속상하고 슬프셨겠어요. 부모님께 혼나서 슬픈 마음이 풀릴 수 있을까요?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 알파카 탬플릿 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b24e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 저녁 메뉴 추천해줘.\n",
      "\n",
      "A: 오늘 저녁 메뉴로 추천드리는 것은 \"돼지갈비찜\"입니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 챗 탬플릿 감정 대화 50 스텝 학습\n",
    "question = \"오늘 저녁 메뉴 추천해줘.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cd09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 부모님께 혼나서 기분이 좋지 않으신 것 같아요.\n",
      "무슨 일이 있었나요?\n",
      "부모님께 혼난 이유를 말씀해주실 수 있나요?\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바랄게요.\n",
      "부모님과의 대화에서 기분이 나아지시길 바�\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 챗 탬플릿 assistant only 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d95e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 많이 속상하시겠어요.\n",
      "오늘 시험이 왜 그런 일이 생겼나요?\n",
      "그러한 일이 생기면 어떻게 하면 좋을까요?\n",
      "부모님께 속상한 마음을 말씀드릴 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "어떤 방법으로 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "어떤 방법으로 기분을 풀 수 있을까요?\n",
      "또한 부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "또한 부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "부모님과의 대화를 통해 기분을 풀 수 있을까요?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 챗 탬플릿 assistant only 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f919f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 어머, 시험을 망쳐서 많이 슬프겠네요. 그래도 부모님께 혼나서 많이 혼나셨겠어요. 기분이 나아지도록 어떻게 하면 좋을까요? 같이 이야기 나눠봐요. 그래도 부모님께서 기운 내라고 응원해 주실 거예요. 힘내세요!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 알파카 탬플릿 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4952dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\n",
      "\n",
      "A: 그런 상황이 참 안타깝네요. 시험 결과는 누구에게나 힘들 수 있는 부분이지만, 그로 인해 부모님께 혼나는 것은 정말 마음 아플 것 같아요. \n",
      "\n",
      "부모님께서도 너의 감정을 이해하려고 노력하시면 좋을 것 같아요. 혼내시더라도 시험 결과에 대한 구체적인 피드백을 주시고, 앞으로 어떻게 개선할 수 있을지 함께 고민해보면 좋겠어요. \n",
      "\n",
      "또한, 스스로를 너무 비난하지 말고, 다음에는 더 잘할 수 있을 거라는 자신감을 가지는 것도 중요해요. 힘든 시간이겠지만, 주변 사람들의 지지와 격려가 큰 힘이 될 거예요.\n",
      "\n",
      "혹시 친구나 선생님과 이야기하고 싶다면, 그들도 너를 이해하고 도와줄 수 있을 거예요. 슬픈 감정을 털어놓고 함께 해결책을 찾아보는 것도 좋은 방법이에요.\n",
      "\n",
      "기분이 나아질 수 있는 방법을 찾아보거나, 좋아하는 활동을 하면서 기분을 전환하는 것도 도움이 될 수 있어요. \n",
      "\n",
      "언제든지 이야기할 수 있는 사람이 있다는 것을 기억하고, 힘든 시간을 잘 이겨내길 바랍니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kanana-nano-2.1b-instruct 알파카 탬플릿 감정 대화 50 스텝 학습\n",
    "question = \"오늘 시험을 망쳐서 부모님께 혼났어. 너무 슬퍼.\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "response = stream_with_langchain(question)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456947d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
